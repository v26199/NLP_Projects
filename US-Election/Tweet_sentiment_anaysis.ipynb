{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNTThr5jJ6im"
   },
   "source": [
    "# Twitter sentiment analysis using NLP techniques\n",
    "### NLP for sentiment analysis of tweets: demo for using the most popular libraries\n",
    "\n",
    "#### It's not any text data, though. I'll be using a dataset of tweets, collected for a period of three weeks around the 2020 US presidential elections.\n",
    "#### DataLink :- https://www.kaggle.com/datasets/manchunhui/us-election-2020-tweets/data\n",
    "#### I downloaded the dataset directly from the link above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvuC5QMKLoeu"
   },
   "source": [
    "\n",
    "# Optimizing Political Campaigns with Twitter Sentiment Analysis:\n",
    "\n",
    "**Step-by-Step Approach**\n",
    "\n",
    "First, I conduct classic exploratory data analysis to understand basic questions like the number of tweets and the date range. Next, I perform text pre-processing to clean the data by removing stop words and converting words to their base forms (e.g., plurals to singular).\n",
    "\n",
    "I then carry out sentiment analysis using three libraries: TextBlob, VADER, and Flair, and compare their results to determine which one best suits our dataset.\n",
    "\n",
    "Given the context of a political campaign, I aim to derive actionable insights from the sentiment analysis results. The goal is to use Twitter sentiment analysis to find strategies to increase our candidate's voter base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqSBTtTZMBQY"
   },
   "source": [
    "<html><center><h1> Table of contents</h1>\n",
    "\n",
    "Exploratory Data Analysis\n",
    "\n",
    "Text pre-processing\n",
    "\n",
    "Intro to sentiment analysis\n",
    "\n",
    "Sentiment analysis with TextBlob\n",
    "\n",
    "Sentiment analysis with VADER\n",
    "\n",
    "Sentiment analysis with Flair\n",
    "\n",
    "Which is the best sentiment analysis library ?\n",
    "\n",
    "Actionable insights from sentiment analysis of tweets\n",
    "\n",
    "</center></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nASd7h9PKV2"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VLhcBwiAKHT8"
   },
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTuGzAAotz7g",
    "outputId": "887d4082-f4cb-4ce1-b4bc-c568c69879be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
     ]
    }
   ],
   "source": [
    "# stopwords, tokenizer, stemmer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re # regular expressions\n",
    "\n",
    "# !pip install gensim\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords # we also use gensim for stopwords removal\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EfAElq8x2k3C"
   },
   "outputs": [],
   "source": [
    "# !pip install flair\n",
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('I love Berlin .')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = Classifier.load('sentiment')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print the sentence with all annotations\n",
    "print(sentence)\n",
    "from segtok.segmenter import split_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIkgseSlKN62"
   },
   "outputs": [],
   "source": [
    "# Loading each dataset\n",
    "trump_df = pd.read_csv('/content/hashtag_donaldtrump .csv', lineterminator='\\n')\n",
    "biden_df = pd.read_csv('/content/hashtag_joebiden .csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQBrCWLT2jpw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZopcbCOpNfku"
   },
   "outputs": [],
   "source": [
    "print('Total number of records in Trump dataset: ', trump_df.shape)\n",
    "print('Total number of records in Biden dataset: ', biden_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHaTUpxyPZNB"
   },
   "outputs": [],
   "source": [
    "trump_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3t77hpiPfq_"
   },
   "outputs": [],
   "source": [
    "biden_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpVC6-S6Pjv2"
   },
   "outputs": [],
   "source": [
    "trump_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5-wJN1jPoVS"
   },
   "outputs": [],
   "source": [
    "# lets get overall idea about the data by using profile report fearture from pandas.\n",
    "!pip install ydata-profiling\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlDBM03HQMKY"
   },
   "outputs": [],
   "source": [
    "trump_profile_report = ProfileReport(trump_df, title=\"Profiling Report\")\n",
    "biden_profile_report = ProfileReport(biden_df, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD674-J_QU3v"
   },
   "outputs": [],
   "source": [
    "trump_profile_report.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4fx9AKHQqLJ"
   },
   "outputs": [],
   "source": [
    "# to view as html file.\n",
    "biden_profile_report.to_file(\"biden_profile_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ-jcA2KSDIt"
   },
   "source": [
    "#### Observation:- By using this wonderful tool Profilling we got this details data understanding with just few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AX3briZhQ8Ju"
   },
   "outputs": [],
   "source": [
    "# Remove unneeded columns\n",
    "\n",
    "irrelevant_columns = ['source','user_name','user_screen_name','user_description','user_join_date','collected_at']\n",
    "\n",
    "# dump this all unwanted features.\n",
    "\n",
    "trump_df = trump_df.drop(columns=irrelevant_columns)\n",
    "biden_df = biden_df.drop(columns=irrelevant_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLhrARXWSnaU"
   },
   "outputs": [],
   "source": [
    "# check for missing values.\n",
    "trump_df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0EqnPkzTFRD"
   },
   "outputs": [],
   "source": [
    "biden_df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxSugVl_TNPi"
   },
   "outputs": [],
   "source": [
    "# lets drop these for now.\n",
    "\n",
    "trump_df = trump_df.dropna()\n",
    "biden_df = biden_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3eyHH-ITS0i"
   },
   "outputs": [],
   "source": [
    "biden_df.country.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tlDwYszTivQ"
   },
   "source": [
    "#### As we don't need out country for the voting on election. We will drop all of them and only focus on USA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XX-tymu-ThPE"
   },
   "outputs": [],
   "source": [
    "trump_usa_df = trump_df[trump_df.country == \"United States of America\"]\n",
    "biden_usa_df = biden_df[biden_df.country == \"United States of America\"]\n",
    "\n",
    "del trump_df\n",
    "del biden_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUaOuVhjTzXd"
   },
   "outputs": [],
   "source": [
    "print('Total number of records in Trump USA dataset: ', trump_usa_df.shape)\n",
    "print('Total number of records in Biden USA dataset: ', biden_usa_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hOXwTe0UIIn"
   },
   "source": [
    "#### Duplicate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lFZRlnbT2lX"
   },
   "outputs": [],
   "source": [
    "tids = trump_usa_df.tweet_id\n",
    "bids = biden_usa_df.tweet_id\n",
    "\n",
    "ids_tweets_in_common = set(trump_usa_df.tweet_id).intersection(set(biden_usa_df.tweet_id))\n",
    "len(ids_tweets_in_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX6j5imXUbUD"
   },
   "source": [
    "Around 20.000 tweets show up in both dataset. I think it doesn't make sense for the same tweet to contribute to compute the sentiment towards Biden and towards Trump. Let's say one tweet has a negative emotion, do we know if it's negative towards both candidates or only towards one of them and just mentions the second ?\n",
    "Let's have a look at a few of the 'duplicate' tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoZBWRdJUXTr"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000 #by default, Python will likely display only the first 50 characters from a long text\n",
    "\n",
    "biden_usa_df.tweet.loc[biden_usa_df.tweet_id.isin(list(ids_tweets_in_common))].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyje1ST0UkbI"
   },
   "source": [
    "From the tweets above, it looks like the duplicate tweets are not really helpful for a per candidate sentiment analysis, so I'll drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1ylI91yUfe6"
   },
   "outputs": [],
   "source": [
    "trump_usa_unique_df = trump_usa_df[~trump_usa_df['tweet_id'].isin(ids_tweets_in_common)]\n",
    "biden_usa_unique_df = biden_usa_df[~biden_usa_df['tweet_id'].isin(ids_tweets_in_common)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZSscbC1Unly"
   },
   "outputs": [],
   "source": [
    "print('Total number of unique records in Trump USA dataset: ', trump_usa_unique_df.shape)\n",
    "print('Total number of unique records in Biden USA dataset: ', biden_usa_unique_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUrrXjpPUpul"
   },
   "outputs": [],
   "source": [
    "trump_usa_unique_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiKSkqnyVLQI"
   },
   "source": [
    "And the second observation is that we have Tweets in Neglish and Spanish too. Most Natural Language Processing libraries can only handle a single language. So we will keep only the tweets in English.\n",
    "\n",
    "I tried several of the packages that can handle language detection which are mentioned in this :- https://stackoverflow.com/questions/39142778/how-to-determine-the-language-of-a-piece-of-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZndJNFAVImc"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect\n",
    "\n",
    "# testing\n",
    "example_tweet = '#Wisconsin podría ser el punto de inflexión en la carrera entre #Trump y #Biden https://t.co/WFf8A1hAn7'\n",
    "\n",
    "# !pip install langdetect\n",
    "\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Example tweet\n",
    "example_tweet = \"Bonjour tout le monde\"\n",
    "\n",
    "# Detect language using langdetect\n",
    "detected_language = detect(example_tweet)\n",
    "print(f'Language of text { example_tweet} is: {detected_language}')\n",
    "\n",
    "# Proceed with TextBlob analysis\n",
    "b = TextBlob(example_tweet)\n",
    "print(f'Sentiment: {b.sentiment}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Afa0wvlWMLa"
   },
   "outputs": [],
   "source": [
    "#try out langdetect on a sample tweet\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "detect(\"#Wisconsin podría ser el punto de inflexión en la carrera entre #Trump y #Biden https://t.co/WFf8A1hAn7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1fd3mBhdT8G"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def get_language(tweet):\n",
    "    try:\n",
    "        lang=detect(tweet)\n",
    "    except:\n",
    "        lang='no'\n",
    "        # for some tweets, detect will throw an error.\n",
    "        # uncomment the line below if you want to look further into this behavior\n",
    "        #print(\"This tweet throws an error:\", tweet)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqiMmxyOdjT_"
   },
   "source": [
    "From my initial tests with TextBlob I saw that language detection will take a long time. Let's first try it on 1.000 records to get an idea of how long we'll have to wait for our full datasets language analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F75G4En6dhov"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "test_df = trump_usa_unique_df.iloc[:1000].copy()\n",
    "test_df['lang'] = test_df.tweet.apply(lambda x: get_language(x))\n",
    "\n",
    "stop_time = time.time()\n",
    "\n",
    "print(f'It took {np.around((time.time() - start_time),decimals=1)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sqvjEvJdmYb"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "trump_usa_unique_df['lang'] = trump_usa_unique_df.tweet.apply(lambda x: get_language(x))\n",
    "\n",
    "stop_time = time.time()\n",
    "print(f'It took {np.around((time.time() - start_time), decimals=1)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM7PIyaVZqay"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "biden_usa_unique_df['lang'] = biden_usa_unique_df.tweet.apply(lambda x: get_language(x))\n",
    "\n",
    "stop_time = time.time()\n",
    "print(f'It took {np.around((time.time() - start_time), decimals=1)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuhCFusmZsyf"
   },
   "outputs": [],
   "source": [
    "biden_usa_unique_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIT-3_H6auE-"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "ax = biden_usa_unique_df.lang.value_counts().plot.bar(rot=0)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "plt.title('Frequency of languages in Biden tweets')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "ax = trump_usa_unique_df.lang.value_counts().plot.bar(rot=0)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "plt.title('Frequency of languages in Trump tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdVKe2C4bUc2"
   },
   "source": [
    "#### We keep only the tweets in English.\n",
    "First, I'm getting rid of the long names, as they're making it more difficult to follow along, rather than helping, now that they became quite long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVbS-VsWayRy"
   },
   "outputs": [],
   "source": [
    "biden_df = biden_usa_unique_df.copy()\n",
    "del biden_usa_unique_df\n",
    "\n",
    "trump_df = trump_usa_unique_df.copy()\n",
    "del trump_usa_unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2-jx_SrbX4b"
   },
   "outputs": [],
   "source": [
    "biden_df = biden_df[biden_df.lang == 'en']\n",
    "trump_df = trump_df[trump_df.lang == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qT9I8bbLeADB"
   },
   "outputs": [],
   "source": [
    "biden_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBz99yDUbgCO"
   },
   "outputs": [],
   "source": [
    "print('Total number of records in Trump dataset: ', trump_df.shape)\n",
    "print('Total number of records in Biden dataset: ', biden_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DYIf-oiekHZ"
   },
   "outputs": [],
   "source": [
    "trump_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0scfTh7Pbkp2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (assuming trump_df and biden_df are your DataFrames)\n",
    "# Including a 'lang' column to demonstrate filtering by language\n",
    "trump_data_2 = {'tweets': [\"Trump tweet 1\", \"Trump tweet 2\", \"Trump tweet 3\"], 'lang': ['en', 'es', 'en']}\n",
    "biden_data_2 = {'tweets': [\"Biden tweet 1\", \"Biden tweet 2\", \"Biden tweet 3\"], 'lang': ['en', 'en', 'es']}\n",
    "\n",
    "# Creating DataFrames\n",
    "trump_df = pd.DataFrame(trump_data_2)\n",
    "biden_df = pd.DataFrame(biden_data_2)\n",
    "\n",
    "# Initial counts before filtering\n",
    "trump_initial_count = trump_df.shape[0]\n",
    "biden_initial_count = biden_df.shape[0]\n",
    "\n",
    "# Filtering DataFrames to include only English tweets\n",
    "trump_df = trump_df[trump_df.lang == 'en']\n",
    "biden_df = biden_df[biden_df.lang == 'en']\n",
    "\n",
    "# Print the retained percentage\n",
    "print(f'We retained {np.around(trump_df.shape[0] * 100 / trump_initial_count, decimals=1)}% of the initial Trump dataset')\n",
    "print(f'And {np.around(biden_df.shape[0] * 100 / biden_initial_count, decimals=1)}% from Biden')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_kkJLRsdVvj"
   },
   "source": [
    "## Analys the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIYO8rtEbrGV"
   },
   "outputs": [],
   "source": [
    "trump_df['ds'] = 'trump'\n",
    "biden_df['ds'] = 'biden'\n",
    "\n",
    "# Combine the filtered on United States Trump and Biden Datasets\n",
    "tweets_df = pd.concat([biden_df, trump_df],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqbdfXS6dYE1"
   },
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NorQFFxdzxC"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "tweets_df.created_at.dt.date.value_counts().sort_index().plot.bar(rot=90, alpha=0.3,color='green')\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "plt.title('Frequency of tweets per day')\n",
    "plt.show()\n",
    "\n",
    "stop_time = time.time()\n",
    "print(f'It took {(time.time() - start_time)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAerLPCYf1CF"
   },
   "outputs": [],
   "source": [
    "most_popular_tweet = tweets_df.loc[tweets_df['retweet_count'].idxmax()]\n",
    "print(f\" The tweet:\\n'{most_popular_tweet.tweet}'\\nwas retweeted the most ({most_popular_tweet.retweet_count} times).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hF-S23ZugNgS"
   },
   "source": [
    "#### So, our most popular tweet contains irony. I'm curious to see how sentiment analysis libraries will perform on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1z8EV9Mf7T8"
   },
   "outputs": [],
   "source": [
    "tweets_df[['tweet_id','user_id','created_at', 'likes', 'retweet_count', 'tweet', 'ds']].iloc[tweets_df.retweet_count.sort_values(ascending=False).head(5).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSaRAvZcgQ5d"
   },
   "outputs": [],
   "source": [
    "print(f'Our 2nd most popular tweet was retweeted for number of times equal to {np.around(13500*100/tweets_df.shape[0], decimals=1)}% of our dataset size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iy3N6tw9gWpP"
   },
   "outputs": [],
   "source": [
    "new_var = tweets_df[tweets_df.tweet.str.contains('Are you there')][['created_at', 'tweet', 'user_id']]\n",
    "new_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bezU8NYgb6j"
   },
   "outputs": [],
   "source": [
    "# tweets_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCKUcJ07gg_q"
   },
   "outputs": [],
   "source": [
    "print(f'There are {tweets_df.retweet_count.nunique()} different amounts of retweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQA5b7-2g_U-"
   },
   "source": [
    "\n",
    "#### In the kdeplot below we explore the retweeting behavior. It looks like a huge amount of tweets are never retweeted. And then we have a tiny number of tweets that get retweeted all the way up to ~ 17.500 times. This was to be expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oyF9E3Ag6Q9"
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(x='retweet_count', data=tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCz2BYe5hZWS"
   },
   "source": [
    "#### \"Vocal minority\" and \"silent majority\" effect¶\n",
    "Mustafaraj et al. 2011 [1] showed evidence of the existance on social media of a minority of users which are very vocal, while there is a majority of users which hardly produce content.\n",
    "\n",
    "We explore this phenomenon in our dataset by looking at the distribution of the number of tweets per user.\n",
    "The frequency distribution we obtain confirms that there are a small number of users producing a large portion of the tweets for both candidates (the trend is stronger for Biden).\n",
    "This indicates that:\n",
    "\n",
    "drawing conclusions about which candidate is preferred based on the number of tweets would be influenced strongly by this small number of very active users.\n",
    "In the section dedicated to 'predicting' election results from Tweets, we will see how we can enforce a policy of 'one vote per person' when analysing tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqMeecHrhD7c"
   },
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(1,1, figsize=(12,6))\n",
    "\n",
    "ax.set_title('Frequency distribution of number of tweets per user', fontsize = 16)\n",
    "sns.kdeplot(trump_df.groupby(['user_id'])['tweet'].count(), shade=True, color='r', label='Trump', ax = ax)\n",
    "sns.kdeplot(biden_df.groupby(['user_id'])['tweet'].count(), shade=True, color='b', label='Biden', ax = ax)\n",
    "labels= [\"Trump\", \"Biden\"]\n",
    "ax.legend(labels)\n",
    "#ax.set_ylim(0, .005)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW9WuKZKhuE6"
   },
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6riayjXhjA0"
   },
   "outputs": [],
   "source": [
    "tweets_df.tweet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9GzWrL0hpfI"
   },
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym5fxULOh9Yh"
   },
   "source": [
    "#### First, we clean our data:\n",
    "\n",
    "we convert everything to lowercase\n",
    "\n",
    "we remove punctuation, links, @mentions and # hashtags\n",
    "\n",
    "we remove stop words - stop words are a set of commonly used words in any language.\n",
    "For example, in English, “the”, “is” and “and”. These don't add any meaningful information for our analysis\n",
    "lemmatization - reduces inflected words to the root of that word (e.g. 'pursuing' becomes 'pursue')\n",
    "\n",
    "tokenization - split each tweet into a list of individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZrZhyJth2PV"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def clean_text(tweet, lemmatize='nltk'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    tweet - a string representing the text we need to clean\n",
    "    lemmatize - one of two possible values {spacy, nltk}\n",
    "      two lemmatization methods\n",
    "      with our dataset, we got the best results with nltk\n",
    "      but Spacy also did a good job, hence you might\n",
    "      try both and compare results for your own data\n",
    "\n",
    "    Output:\n",
    "    tokenized - the cleaned text, tokenized (a list of string words)\n",
    "    \"\"\"\n",
    "    tweet = tweet.lower() # lowercase\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE) # remove urls\n",
    "    tweet = re.sub(r'\\@\\w+|\\#', '', tweet) # remove mentions of other usernames and the hashtag character\n",
    "    tweet = remove_stopwords(tweet) # remove stopwords with Gensim\n",
    "\n",
    "    if lemmatize == 'spacy':\n",
    "        # Initialize spacy 'en_core_web_sm' model, keeping only tagger component needed for lemmatization\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "        doc = nlp(tweet)\n",
    "        tokenized = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    elif lemmatize == 'nltk':\n",
    "        '''\n",
    "        lemmatization works best when WordNetLemmatizer receives both the text and the part of speech of each word\n",
    "        the code below assigns POS (part of speech) tag on a per word basis (it does not infer POS from content / sentence), which might not be optimal\n",
    "        '''\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokenized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(tweet)]\n",
    "\n",
    "    # remove left over stop words with nltk\n",
    "    tokenized = [token for token in tokenized if token not in stopwords.words(\"english\")]\n",
    "\n",
    "    # remove non-alpha characters and keep the words of length >2 only\n",
    "    tokenized = [token for token in tokenized if token.isalpha() and len(token) > 2]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "def combine_tokens(tokenized):\n",
    "    non_tokenized = ' '.join([w for w in tokenized])\n",
    "    return non_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LPwsQSJjKfb"
   },
   "outputs": [],
   "source": [
    "# Cleaning our tweets.\n",
    "\n",
    "start =  time.time()\n",
    "\n",
    "tweets_df['tokenized_tweet_nltk'] = tweets_df['tweet'].apply(lambda x: clean_text(x, 'nltk'))\n",
    "tweets_df['clean_tweet_nltk'] = tweets_df['tokenized_tweet_nltk'].apply(lambda x: combine_tokens(x))\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Cleaning all tweets takes ~{round((stop-start)/60, 3)} minutes: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GPXuSgljc_W"
   },
   "outputs": [],
   "source": [
    "# save this clean data file\n",
    "tweets_df.to_csv('clean_tweets_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcTA_Ltbkunb"
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df.tweet_id.isin(list(trump_df.tweet_id))].ds = 'trump'\n",
    "tweets_df[tweets_df.tweet_id.isin(list(biden_df.tweet_id))].ds = 'biden'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQJvpmBulAGX"
   },
   "outputs": [],
   "source": [
    "tweets_df.head(10)[['tweet', 'clean_tweet_nltk']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EzuTH64mKuk"
   },
   "source": [
    "# sentiment analysis\n",
    "\n",
    "### The most popular algorithms are:\n",
    "\n",
    "### Rule-based models\n",
    "For example, TextBlob and Vader They use a bag-of-words approach: the text is considered to be the sum of its constituent words,\n",
    "\n",
    "### Word-embedding-based models:\n",
    "Words are represented as vectors of numbers in an n-dimensional space This mapping from individual words to a continuous vector space can be generated through various methods: neural networks, dimensionality reduction, co-occurence matrix.\n",
    "\n",
    "* For this analysis of tweets I tried three of the currently most popular sentiment analysis libraries.\n",
    "TextBlob and Vader use rule-based models, while Flair uses word embeddings.\n",
    "\n",
    "* All three output a continuous number between -1 and 1.\n",
    "If one needs a classification into categories instead of these numerical values, the common interpretation is that <0 is negative, 0 is neutral and >0 is positive. The cutoff points for the three categories are not set in stone and can be adapted based on the results / visual inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7VWFl76nnH0"
   },
   "source": [
    "### Popular libraries for sentiment analysis¶\n",
    "For this analysis of tweets I tried three of the currently most popular sentiment analysis libraries.\n",
    "* TextBlob use rule-based models.\n",
    "* Vader use rule-based models.\n",
    "* while Flair uses word embeddings.\n",
    "\n",
    "All three output a continuous number between -1 and 1.\n",
    "If one needs a classification into categories instead of these numerical values, the common interpretation is that <0 is negative, 0 is neutral and >0 is positive. The cutoff points for the three categories are not set in stone and can be adapted based on the results / visual inspection.\n",
    "\n",
    "## differences between each:\n",
    "\n",
    "**TextBlob** is the simplest of them It does estimate though how factual versus opinionated a text is\n",
    "\n",
    "**Vader** The valence for the words in the dictionary was empirically validated by multiple human judges “especially attuned to microblog-like contexts”\n",
    "Uses some heuristics to recognize word negations (“cool” versus “not cool”) and word intensifiers (“a bit sad” versus “really sad”)\n",
    "Cannot recognize typos and will consider them out of vocabulary words (veri relevant for twitter, where users tend to not spell correctly)\n",
    "\n",
    "**Flair** is a pre-trained character-level LSTM (recurrent neural networks) classifier which takes into account:\n",
    "the sequence of words\n",
    "the sequence of letters -> recognizes typos\n",
    "intensifiers ('so', 'very', ‘a bit’ etc)\n",
    "Flair is trained on IMDB movie reviews dataset and retraining is resource intensive.\n",
    "Very polarizing (assigns very positive or very negative scores), but not much in the middle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D641xlySqJ3A"
   },
   "source": [
    "### 1> Sentiment analysis with TextBlob¶\n",
    "According to TextBlob's official website, TextBlob \"provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\"\n",
    "\n",
    "TextBlob library will output something like this for each snippet of text that it analyzes:\n",
    "Sentiment(polarity=-0.125, subjectivity=0.5916666666666667)\n",
    "\n",
    "That is, TextBlob will output:\n",
    "\n",
    "a measure of polarity, which can have values in the interval [-1, 1]\n",
    "an estimation of subjectivity, ranging is [0.0, 1.0] where 0.0 is very objective (dealing with facts) and 1.0 is very subjective (opinions).\n",
    "\n",
    "We will adopt the approach like:-\n",
    "\n",
    "we will label\n",
    "* <0 values as 'negative'\n",
    "* 0 values as 'neutral'\n",
    "* and >0 values as 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgZDaZqYlEtO"
   },
   "outputs": [],
   "source": [
    "# Helper Function to assign Label for Sentiment Analysis with TextBlob\n",
    "def create_sentiment_labels(df, feature,value):\n",
    "    '''\n",
    "    in:\n",
    "        dataframe\n",
    "        value on which to classify\n",
    "        feature - column name of the feature that receives the label\n",
    "    out:\n",
    "        does not return a value\n",
    "        modifies the dataframe received as parameter\n",
    "    '''\n",
    "\n",
    "    df.loc[df[value] > 0,feature] = 'positive'\n",
    "    df.loc[df[value] == 0,feature] = 'neutral'\n",
    "    df.loc[df[value] < 0,feature] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RpoXaI9qfbN"
   },
   "outputs": [],
   "source": [
    "# Polarity and subjectivity\n",
    "def sentiment_analysis(dataframe):\n",
    "    dataframe['blob_polarity'] = dataframe['clean_tweet_nltk'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    dataframe['blob_subjectivity'] = dataframe['clean_tweet_nltk'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "    create_sentiment_labels(dataframe, 'blob_sentiment','blob_polarity')\n",
    "\n",
    "    return dataframe[['clean_tweet_nltk','blob_polarity','blob_subjectivity','blob_sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5QMrZZJqqj6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start =  time.time()\n",
    "\n",
    "sentiment_analysis(tweets_df)\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Sentiment analysis with TextBlob took: {round((stop-start)/60, 3)} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HphzGNjaqvAf"
   },
   "outputs": [],
   "source": [
    "tweets_df.head(10)[['blob_polarity','blob_subjectivity', 'blob_sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Trr-7BUrV2f"
   },
   "source": [
    "### Different Ways to Look at the Same Data\n",
    "\n",
    "#### Option A: Average Polarity Per Candidate\n",
    "\n",
    "To analyze sentiment, we can compute the average polarity (sentiment score) for each candidate by averaging the polarity of all tweets about them using TextBlob.\n",
    "\n",
    "#### Issues with This Approach\n",
    "\n",
    "Consider this scenario:\n",
    "- One user tweets 99 times with a polarity of -1 (negative sentiment).\n",
    "- Another user tweets once with a polarity of 1 (positive sentiment).\n",
    "\n",
    "If we average the polarity across all tweets, we get a result of -0.98.\n",
    "\n",
    "#### Problems with the Result\n",
    "- This result suggests strong opposition to the candidate.\n",
    "- However, it doesn't accurately reflect that we have one supporter and one opposer in our sample.\n",
    "- The average only tells us the overall sentiment across tweets, not the true distribution of support and opposition.\n",
    "\n",
    "In summary, averaging polarity might mislead us about the actual sentiment distribution among users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPgcRmNIqzTR"
   },
   "outputs": [],
   "source": [
    "#update the divided dataset\n",
    "trump_df = tweets_df[tweets_df.ds=='trump']\n",
    "biden_df = tweets_df[tweets_df.ds=='biden']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,5))\n",
    "\n",
    "fig.suptitle('TextBlob analysis: nmean polarity (-1.0, 1.0) and mean subjectivity (0.0, 1.0) per candidate (one tweet, one sentiment)', fontsize=14)\n",
    "\n",
    "features = ['blob_polarity', 'blob_subjectivity']\n",
    "values = [trump_df.groupby(['user_id'])['blob_polarity'].mean().mean(), trump_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()]\n",
    "axes[0].bar(features,values, width=0.2)\n",
    "axes[0].set_ylim(0, .5)\n",
    "axes[0].set_title('Trump', fontsize = 14)\n",
    "axes[0].set_ylabel('Value', fontsize = 12)\n",
    "\n",
    "values = [biden_df.groupby(['user_id'])['blob_polarity'].mean().mean(), biden_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()]\n",
    "axes[1].bar(features,values, width=0.2)\n",
    "axes[1].set_ylim(0, .5)\n",
    "axes[1].set_title('Biden', fontsize = 14)\n",
    "axes[1].set_ylabel('Value', fontsize = 12)\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.88])\n",
    "plt.show()\n",
    "\n",
    "trump_usa_pol_tweet =trump_df['blob_polarity'].mean()\n",
    "trump_usa_subj_tweet = trump_df['blob_subjectivity'].mean()\n",
    "biden_usa_pol_tweet = biden_df['blob_polarity'].mean()\n",
    "biden_usa_subj_tweet = biden_df['blob_subjectivity'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALukMNcVrmXO"
   },
   "source": [
    "#### Option b\n",
    "Another option is to:\n",
    "\n",
    "first average sentiment expressed through tweets per user id -> we will have one averge expressed sentiment per user per candidate\n",
    "then average across the whole population for each candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzEdlzYOraFy"
   },
   "outputs": [],
   "source": [
    "# the below gives us a mean per user\n",
    "# trump_usa_df[['user_id', 'Polarity']].groupby(['user_id'])['Polarity'].mean()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 5))\n",
    "\n",
    "fig.suptitle('TextBlob analysis: mean polarity (-1.0, 1.0) and mean subjectivity (0.0, 1.0)\\nper candidate (one user, one sentiment)', fontsize=14)\n",
    "\n",
    "features = ['blob_polarity', 'blob_subjectivity']\n",
    "values = [trump_df.groupby(['user_id'])['blob_polarity'].mean().mean(), trump_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()]\n",
    "axes[0].bar(features,values, width=0.2,)\n",
    "axes[0].set_ylim(0, .5)\n",
    "axes[0].set_title('Trump', fontsize = 14)\n",
    "axes[0].set_ylabel('Value', fontsize = 12)\n",
    "\n",
    "values = [biden_df.groupby(['user_id'])['blob_polarity'].mean().mean(), biden_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()]\n",
    "axes[1].bar(features,values, width=0.2,)\n",
    "axes[1].set_ylim(0, .5)\n",
    "axes[1].set_title('Biden', fontsize = 14)\n",
    "axes[1].set_ylabel('Value', fontsize = 12)\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.88])\n",
    "plt.show()\n",
    "\n",
    "trump_usa_pol_user = trump_df.groupby(['user_id'])['blob_polarity'].mean().mean()\n",
    "trump_usa_subj_user = trump_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()\n",
    "biden_usa_pol_user = biden_df.groupby(['user_id'])['blob_polarity'].mean().mean()\n",
    "biden_usa_subj_user = biden_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryQw3NOorscJ"
   },
   "outputs": [],
   "source": [
    "#how our results are influenced by choosing either of the two options mentioned above\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,6))\n",
    "\n",
    "fig.suptitle('TextBlob analysis: \\nmean polarity and mean subjectivity\\n (tweet level = one tweet, one sentiment) vs (user level = one user, one sentiment)', fontsize=16)\n",
    "\n",
    "#features = ['Polarity', 'Subjectivity']\n",
    "features = np.array([1, 2])\n",
    "values_tweet = [ trump_usa_pol_tweet, trump_usa_subj_tweet]\n",
    "values_user = [ trump_usa_pol_user, trump_usa_subj_user]\n",
    "\n",
    "#values = [[trump_usa_pol_tweet, trump_usa_subj_tweet],\n",
    "#[trump_usa_pol_user, trump_usa_subj_user]]\n",
    "\n",
    "axes[0].bar(features-0.2, values_tweet, width=0.2, align = 'center', color = 'y')\n",
    "axes[0].bar(features, values_user, width=0.2, align = 'center', color = 'g')\n",
    "#axes[0].bar(features,values)\n",
    "axes[0].set_ylim(0, .5)\n",
    "axes[0].set_title('Trump', fontsize = 16)\n",
    "axes[0].set_xlabel('Feature', fontsize = 14)\n",
    "axes[0].set_ylabel('Average value', fontsize = 14)\n",
    "axes[0].set_xticklabels(['', 'Polarity', '', '', '', '', '', 'Subjectivity'])\n",
    "labels= [\"tweet level\", \"user level\"]\n",
    "axes[0].legend(labels)\n",
    "\n",
    "values_tweet = [ biden_usa_pol_tweet, biden_usa_subj_tweet]\n",
    "values_user = [ biden_usa_pol_user, biden_usa_subj_user]\n",
    "axes[1].bar(features-0.2,values_tweet, width=0.2, align = 'center', color = 'y')\n",
    "axes[1].bar(features,values_user, width=0.2, align = 'center', color = 'g')\n",
    "axes[1].set_ylim(0, .5)\n",
    "axes[1].set_title('Biden', fontsize = 16)\n",
    "axes[1].set_xlabel('Feature', fontsize = 14)\n",
    "axes[1].set_ylabel('Average value', fontsize = 14)\n",
    "axes[1].set_xticklabels(['', 'Polarity', '', '', '', '', '', 'Subjectivity'])\n",
    "\n",
    "labels= [\"tweet level\", \"user level\"]\n",
    "axes[1].legend(labels)\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.88])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbDYUHOWsFVQ"
   },
   "source": [
    "#### It doesn't really matter how we average the sentiment. In any case, \"one user one sentiment\" makes much more sense for our analysis. But it's easier to code the \"one tweet one sentiment\", so we'll use this one, since it has the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oirtr6G9r3tK"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_title('--Relative--\\nTextBlob sentiment analysis - \\nrelative frequency per valence type for each candidate', fontsize=16)\n",
    "\n",
    "features = np.array([1,2,3])\n",
    "trump = (trump_df['blob_sentiment'].sort_values().value_counts()/trump_df['blob_sentiment'].shape[0])[['negative', 'neutral', 'positive']]\n",
    "ax.bar(features-0.3, trump.values, width=0.3, align = 'center', color = 'r', alpha= .6)\n",
    "\n",
    "biden = (biden_df['blob_sentiment'].sort_values().value_counts()/biden_df['blob_sentiment'].shape[0])[['negative', 'neutral', 'positive']]\n",
    "ax.bar(features, biden.values, width=0.3, align = 'center', color = 'b', alpha= .6)\n",
    "\n",
    "ax.set_ylim(0, .5)\n",
    "ax.set_xlabel('Valence', fontsize = 14)\n",
    "ax.set_ylabel('Relative frequency', fontsize = 14)\n",
    "\n",
    "ax.set_xticklabels(['', '', 'Negative', '', 'Neutral', '', 'Positive'])\n",
    "\n",
    "labels= [\"Trump\", \"Biden\"]\n",
    "ax.legend(labels)\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.88])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1G5BAfRsTN7"
   },
   "source": [
    "### Observations from the above plot:\n",
    "\n",
    "the ratio of positive:negative is higher for Biden than for Trump.\n",
    "\n",
    "When people tweet about Biden, they tend to be less negative than when they tweet about Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "husySrQTsIoV"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_title('--Absolute--\\nTextBlob sentiment analysis - \\nabsolute frequency per valence type for each candidate', fontsize=16)\n",
    "\n",
    "features = np.array([1,2,3])\n",
    "trump = (trump_df['blob_sentiment'].sort_values().value_counts())[['negative', 'neutral', 'positive']]\n",
    "ax.bar(features-0.3, trump.values, width=0.3, align = 'center', color = 'r', alpha=0.6)\n",
    "\n",
    "biden = (biden_df['blob_sentiment'].sort_values().value_counts())[['negative', 'neutral', 'positive']]\n",
    "ax.bar(features, biden.values, width=0.3, align = 'center', color = 'b', alpha = 0.6)\n",
    "\n",
    "#ax.set_ylim(0, .5)\n",
    "ax.set_xlabel('Valence', fontsize = 14)\n",
    "ax.set_ylabel('Absolute frequency', fontsize = 14)\n",
    "\n",
    "ax.set_xticklabels(['', '', 'Negative', '', 'Neutral', '', 'Positive'])\n",
    "\n",
    "labels= [\"Trump\", \"Biden\"]\n",
    "ax.legend(labels)\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.88])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcjuvE8tsi3d"
   },
   "source": [
    " The absolute frequency plot is relevant because all those negative tweets could potentially be support votes for the other candidate, since in presidential elections people only have 2 options. If they hate one candidate, that could be enough reason to vote for the other one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0xUpTf3tNZE"
   },
   "source": [
    "\n",
    "## 2> Sentiment analysis with VADER¶\n",
    "VADER (Valence Aware Dictionary for Sentiment Reasoning) was developed in 2014.\n",
    "You can check Vader's official github for details of how the tool was designed and how to use it.\n",
    "\n",
    "According to VADER's github, VADER is \"Empirically validated by multiple independent human judges, VADER incorporates a \"gold-standard\" sentiment lexicon that is especially attuned to microblog-like contexts.\"\n",
    "\n",
    "* Vader is a pre-trained model. If you want to read about the model in detail, the official website recommends [2]\n",
    "\n",
    "* Vader outputs something like this:\n",
    "{'neg': 0.0, 'neu': 0.436, 'pos': 0.564, 'compound': 0.3802}\n",
    "\n",
    "Negative, neutral and positive are scores between 0 and 1.\n",
    "The compound value reflects the overall sentiment of the text. It's computed based on the values of negative, neutral and positive. It ranges from -1 (maximum negativity) to 1 (maximum positivity).\n",
    "\n",
    "The is no standard way to interpret compound. One can decide that whatever is larger than 0 is positive and lower is negative, while 0 means neutral.\n",
    "But we can also decide to look only at more extreme values, like above or below +/- 0.8, for example.\n",
    "It really depends on the kind of data you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEwH54-CsZFo"
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1-FGTlftagx"
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis_vader(df, clean = True):\n",
    "    if clean:\n",
    "        target_col = 'clean_tweet_nltk'\n",
    "        prefix = 'vader_clean_'\n",
    "    else:\n",
    "        target_col = 'tweet'\n",
    "        prefix = 'vader_'\n",
    "\n",
    "    scores_col = prefix+'scores'\n",
    "\n",
    "    #let's make it vader_sentiment, so that it has the same naming convention as TextBlob and Flair sentiment score\n",
    "    #compound_col = prefix+'compound'\n",
    "    compound_col = prefix+'polarity'\n",
    "\n",
    "    #comp_score_col = prefix+'comp_score'\n",
    "    comp_score_col = prefix+'sentiment'\n",
    "\n",
    "    df[scores_col] = df[target_col].apply(lambda tweet: sid.polarity_scores(tweet))\n",
    "    df[compound_col] = df[scores_col].apply(lambda d: d['compound'])\n",
    "    create_sentiment_labels(df,comp_score_col,compound_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "923Zfs10uIt7"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "sentiment_analysis_vader(tweets_df)\n",
    "sentiment_analysis_vader(tweets_df, clean = False)\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Sentiment analysis with VADER took: {round((stop-start)/60, 3)} minutes')\n",
    "\n",
    "#update the divided dataset\n",
    "trump_df = tweets_df[tweets_df.ds=='trump']\n",
    "biden_df = tweets_df[tweets_df.ds=='biden']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GumYE3ZIuRr-"
   },
   "source": [
    "Does it matter if we clean the tweets before feeding them to Vader ? Does Vader itself perform a good enough cleaning ? We answer this question by classifying tweets into positive / neutral / negative using both approaches and then looking at the accuracy_score for the labels obtained through the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nG1tCLscuLla"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print(f\"Accuracy score for our cleaning vs vader tweet cleaning for Trump: {accuracy_score(trump_df['vader_sentiment'],trump_df['vader_clean_sentiment']):.4}\")\n",
    "print(f\"Accuracy score for our cleaning vs vader tweet cleaning for Biden: {accuracy_score(biden_df['vader_sentiment'],biden_df['vader_clean_sentiment']):.4}\")\n",
    "\n",
    "stop = time.time()\n",
    "print(f'This took: {round((stop-start)/60, 3)} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eIj5IRxufox"
   },
   "source": [
    "It looks like there is 84% consensus for Trump and 88% consensus for Biden for sentiment per tweet when VADER is fed the raw data versus the cleaned data.\n",
    "So that means the decision to feed raw or cleaned data should be given some thought.\n",
    "Since we don't have labelled data, the only way to decide which method is best is by visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBkehSj9udN6"
   },
   "outputs": [],
   "source": [
    "def get_valence_relative_freq(df):\n",
    "    #grouped = df.sort_values('comp_score').groupby(['comp_score'], sort=False)\n",
    "    grouped = df.sort_values('vader_sentiment').groupby(['vader_sentiment'], sort=False)\n",
    "    valence = grouped['vader_sentiment'].value_counts(normalize=False, sort=False)\n",
    "    valence = valence.droplevel(0)\n",
    "    valence = valence / valence.sum()\n",
    "    return valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbLJ2ke5ud9J"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "trump_tmp = get_valence_relative_freq(trump_df)\n",
    "biden_tmp = get_valence_relative_freq(biden_df)\n",
    "\n",
    "#plt.figure(figsize=(8,6))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,5))\n",
    "fig.suptitle('Vader sentiment analysis - \\nrelative frequency per valence type for each candidate', fontsize=16)\n",
    "#fig.tight_layout()\n",
    "\n",
    "#sns.barplot(trump_tmp.index, trump_tmp.values, ax=axes[0])\n",
    "(trump_tmp).plot(kind='bar', ax = axes[0])\n",
    "axes[0].set_title('Trump', fontsize = 16)\n",
    "axes[0].set_xlabel('Valence', fontsize = 14)\n",
    "axes[0].set_ylabel('Relative frequency', fontsize = 14)\n",
    "axes[0].set_ylim(0, .5)\n",
    "\n",
    "#ax2 = sns.countplot(x=\"comp_score\", data=biden_tmp)\n",
    "#sns.barplot(biden_tmp.index, biden_tmp.values,  ax=axes[1])\n",
    "(biden_tmp).plot(kind='bar', ax = axes[1])\n",
    "axes[1].set_title('Biden', fontsize = 16)\n",
    "axes[1].set_xlabel('Valence', fontsize = 14)\n",
    "axes[1].set_ylabel('Relative frequency', fontsize = 14)\n",
    "axes[1].set_ylim(0, .5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIUvmPOZuqdu"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_title('--Relative--\\nTextBlob sentiment analysis - \\nrelative frequency per valence type for each candidate', fontsize=16)\n",
    "\n",
    "features = np.array([1,2,3])\n",
    "\n",
    "trump = get_valence_relative_freq(trump_df)\n",
    "ax.bar(features-0.3, trump.values, width=0.3, align = 'center', color = 'r', alpha= .6)\n",
    "\n",
    "biden = get_valence_relative_freq(biden_df)\n",
    "ax.bar(features, biden.values, width=0.3, align = 'center', color = 'b', alpha= .6)\n",
    "\n",
    "ax.set_ylim(0, .5)\n",
    "ax.set_xlabel('Valence', fontsize = 14)\n",
    "ax.set_ylabel('Relative frequency', fontsize = 14)\n",
    "\n",
    "ax.set_xticklabels(['', '', 'Negative', '', 'Neutral', '', 'Positive'])\n",
    "\n",
    "labels= [\"Trump\", \"Biden\"]\n",
    "ax.legend(labels)\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.88])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPZ-diJ6u61u"
   },
   "source": [
    "### Some observations we can make based on plot above:¶\n",
    "within-candidates:\n",
    "\n",
    "Trump has a ratio of 1:1 for positive to negative tweets, while for Biden, it's almost 2:1\n",
    "\n",
    "between-candidates:\n",
    "higher % of positive tweets for Biden\n",
    "higher % of negative tweets for Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htWU3zinvFFS"
   },
   "source": [
    "### Average sentiment score per candidate¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEbf0XZdu3gT"
   },
   "outputs": [],
   "source": [
    "print(trump_df['vader_polarity'].mean())\n",
    "print(biden_df['vader_polarity'].mean())\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "fig.suptitle('Mean VADER compund score (between -1.0 and 1.0)\\nfor Trump and Biden', fontsize=16)\n",
    "\n",
    "features = ['Mean Valence Trump', 'Mean Valence Biden']\n",
    "values = [trump_df['vader_polarity'].mean(), biden_df['vader_polarity'].mean()]\n",
    "\n",
    "plt.bar(features,values, width=0.2)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim(-.3, .3)\n",
    "axes.set_xlabel('Feature', fontsize = 14)\n",
    "axes.set_ylabel('Value', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0VWqdeCvIlz"
   },
   "outputs": [],
   "source": [
    "# Let's explore further the differences between sentiment for the two candidates.\n",
    "# We continue with visual inspection of the distribution of sentiment scores.\n",
    "\n",
    "bins = 50\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "fig.suptitle('Histograms of tweets polarity per candidate (VADER)', fontsize=16)\n",
    "\n",
    "plt.hist(trump_df['vader_polarity'], bins = bins, alpha = 0.5, color = 'r')\n",
    "plt.hist(biden_df['vader_polarity'], bins = bins, alpha = 0.5, color = 'b')\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim(0, 4000)\n",
    "\n",
    "labels= [\"Trump\", \"Biden\"]\n",
    "axes.legend(labels)\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.88])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO_1mi6PvYP2"
   },
   "source": [
    "## 3> Sentiment analysis with Flair¶\n",
    "Flair is a pre-trained character-level LSTM (recurrent neural networks) classifier which takes into account:\n",
    "\n",
    "* the sequence of words\n",
    "* the sequence of letters\n",
    "* intensifiers ('so', 'very' etc) Advantage over VADER: by looking at character level, it can recognize and correct for typos (e.g. it will recognize that 'anoy' means 'annoy'), which for VADER would just be an OOV (Out Of Vocabulary) word (and thus ignored).\n",
    "\n",
    "##### Advantage over VADER: by looking at character level, it can recognize and correct for typos (e.g. it will recognize that 'anoy' means 'annoy'), which for VADER would just be an OOV (Out Of Vocabulary) word (and thus ignored).\n",
    "\n",
    "Pre-trained Flair models\n",
    "As we lack computing power, we will use a freely available pre-trained Flair model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHi0p00RvMJ6"
   },
   "outputs": [],
   "source": [
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRmK79ZAvwPQ"
   },
   "source": [
    "Remember when we removed from our dataset tweets in other languges than English ? If we had kept them, our 'en' classifier declared above wouldn't have been able to interpret them anyway.\n",
    "\n",
    "Here's an example of how to use Flair classifier to predict for one sentence. Flair clasifier outputs the assigned label and a value between 0 and 1 indicating the confidence level for this prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1oqZ338vrnj"
   },
   "outputs": [],
   "source": [
    "sentence = Sentence('The food was not horrible!')\n",
    "classifier.predict(sentence)\n",
    "\n",
    "print('Sentence above is: ', sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUHAldfIvy98"
   },
   "outputs": [],
   "source": [
    "# Helper functions for performing the sentiment analysis using Flair\n",
    "\n",
    "def flair_make_sentences(text):\n",
    "    \"\"\" Break apart text into a list of sentences \"\"\"\n",
    "    sentences = [sent for sent in split_single(text)]\n",
    "    return sentences\n",
    "\n",
    "def flair_predict_sentences(sentence):\n",
    "    \"\"\" Predict the sentiment of a sentence \"\"\"\n",
    "    if sentence == \"\":\n",
    "        return 0\n",
    "    text = Sentence(sentence)\n",
    "    # stacked_embeddings.embed(text)\n",
    "    classifier.predict(text)\n",
    "    value = text.labels[0].to_dict()['value']\n",
    "    if value == 'POSITIVE':\n",
    "        result = text.to_dict()['labels'][0]['confidence']\n",
    "    else:\n",
    "        result = -(text.to_dict()['labels'][0]['confidence'])\n",
    "    return round(result, 3)\n",
    "\n",
    "def flair_get_scores_per_sentences(sentences):\n",
    "    \"\"\" Call predict on every sentence of a text \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(sentences)):\n",
    "        results.append(flair_predict_sentences(sentences[i]))\n",
    "    results.append(flair_predict_sentences(sentences[0]))\n",
    "    return results\n",
    "\n",
    "def flair_get_sum(scores):\n",
    "    result = round(sum(scores), 3)\n",
    "    return result\n",
    "\n",
    "def flair_get_avg_from_sentences(scores):\n",
    "    result = round(np.mean(scores), 3)\n",
    "    return result\n",
    "\n",
    "def flair_get_score_tweet(text):\n",
    "  if not text:\n",
    "    return 0\n",
    "  s = Sentence(text)\n",
    "  classifier.predict(s)\n",
    "  value = s.labels[0].to_dict()['value']\n",
    "  if value == 'POSITIVE':\n",
    "    result = s.to_dict()['labels'][0]['confidence']\n",
    "  else:\n",
    "    result = -(s.to_dict()['labels'][0]['confidence'])\n",
    "  return round(result, 3)\n",
    "\n",
    "def sentiment_analysis_flair(polarity):\n",
    "  if polarity > 0:\n",
    "    return 'positive'\n",
    "  if polarity == 0:\n",
    "    return 'neutral'\n",
    "  if polarity < 0:\n",
    "    return 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFrJfO1Jv-6t"
   },
   "source": [
    "## Let's explore the Flair results in the context of a comparison between all three methods (TextBlob, VADER and Flair).\n",
    "\n",
    "### Which is the best sentiment analysis library.\n",
    "* TextBlob\n",
    "* VADER\n",
    "* Flair per sentence\n",
    "* Flair per tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qwUxKE-wb4W"
   },
   "source": [
    "### Flair: predict sentiment per sentence versus sentiment per tweet¶\n",
    "\n",
    "But first we will explore and compare sentiment labelling for two ways to use Flair and here is why.\n",
    "We asked oursleved what is the best way to perform prediction for Tweet ?\n",
    "\n",
    "* option 1: Should we predict on the whole Tweet ?\n",
    "* option 2: Should we split into sentences, predict for each sentence and then make an average ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCANFSBDv6Qj"
   },
   "outputs": [],
   "source": [
    "records = 1000\n",
    "temp = tweets_df[tweets_df.ds=='trump'][:records].copy()\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#flair sentiment by diving tweet into sentenes and averaging\n",
    "temp['sentences'] = temp['clean_tweet_nltk'].apply(flair_make_sentences)\n",
    "temp['scores'] = temp['sentences'].apply(flair_get_scores_per_sentences)\n",
    "temp['flair_scores_avg'] = temp.scores.apply(flair_get_avg_from_sentences)\n",
    "\n",
    "#flair sentiment on the whole tweet\n",
    "temp['flair_one_score'] = temp['clean_tweet_nltk'].apply(flair_get_score_tweet)\n",
    "\n",
    "stop = time.time()\n",
    "print(round((stop-start)/60, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9Z9fDQ7wYIw"
   },
   "outputs": [],
   "source": [
    "bins = 50\n",
    "alpha = 0.6\n",
    "fig = plt.plot(figsize=(6,5))\n",
    "\n",
    "plt.title('Flair polarity: per sentence versus per tweet')\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.hist(temp['flair_scores_avg'], bins = bins, alpha = alpha, color = 'r')\n",
    "ax.hist(temp['flair_one_score'], bins = bins, alpha = alpha, color = 'g')\n",
    "\n",
    "ax.set_ylim(0, 100)\n",
    "labels= [\"Flair sentences\", \"Flair tweet\"]\n",
    "ax.legend(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR1kLe6hwwJc"
   },
   "source": [
    "#### The above plot shows that the two methods for computing polarity of a tweet with Flair produce exactly the same results. It might sound silly, but worth trying out because this level of details are not easily accessible from the documentation.\n",
    "\n",
    "\n",
    "### Comparison of the three sentiment analysis libraries¶\n",
    "We only perform this analysis on the 1000 data points because Flair is very resource intensive. But try it out on the whole dataset if you have more computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KWXS-BMwp3C"
   },
   "outputs": [],
   "source": [
    "bins = 50\n",
    "alpha = 0.6\n",
    "fig = plt.plot(figsize=(8,7))\n",
    "\n",
    "plt.title('Distribution of sentiment scores\\nTextBlob vs VADER vs Flair', fontsize=16)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.hist(temp['blob_polarity'], bins = bins, alpha = alpha, color = 'r')\n",
    "ax.hist(temp['vader_polarity'], bins = bins, alpha = alpha, color = '#ffd343')\n",
    "ax.hist(temp['flair_one_score'], bins = bins, alpha = alpha, color = 'g')\n",
    "ax.set_ylim(0, 100)\n",
    "labels= [\"TextBlob\", \"VADER\", \"Flair\"]\n",
    "ax.legend(labels)\n",
    "\n",
    "ax.set_ylabel('Frequency', fontsize = 14)\n",
    "\n",
    "#fig.tight_layout(rect=[0, 0.03, 1, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "050IQCAkwuij"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH0qFy4uw-XW"
   },
   "source": [
    "## Observations from the distribution plot of sentiment produced by TextBlob, VADER and Flair\n",
    "We notice that TextBlob and VADER tend to:\n",
    "\n",
    "classify a lot of the data as neutral\n",
    "VADER has a bimodal distribution, while TextBlob is unimodal.\n",
    "Flair:\n",
    "\n",
    "has no predilection for neutral.\n",
    "and it's extremly polarizing, compared to TextBlob or VADER\n",
    "Regarding the lack of a strong neutral category in Flair (compare it to TextBlob and VADER for example), Flair co-creator, Alan Akbik, explains that when Flair sentiment analysis model was trained on reviews dataset and there was too much variability in people's attitudes in the middle, which prevented the model from learning something useful for a rating that translates to 'average'. Some people would give an average rating if the product/service had a few shortcomings, while others would punish with an average rating if it was a complete disapointment.\n",
    "\n",
    "According to Alan Akbik, they ended up training only on more extreme reviews, to avoid the middle reviews with very low signal to noise ration.\n",
    "That was the best approach for movie reviews. But we don't know if it's the best for our data too, for tweets.\n",
    "\n",
    "Agreement between TextBlob, VADER and Flair predictions\n",
    "We know the three differ in how extreme their predicted sentiment value is. But do they agree on the direction of that sentiment, regardless of whether they agree on intensity. That is, if TexBlob predicts a weak sentiment and Flair a strong sentiment, are they both negative or both positive ? Or one predicts a -.2 (weak negative) and the other a .9 (strong positive) ?\n",
    "\n",
    "To answer the question above, tet's see the percentage of times these algorithms agree with one another when classifying the sentiment of a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEtO3u7Iw-7f"
   },
   "outputs": [],
   "source": [
    "temp['flair_sentiment'] = temp['flair_one_score'].apply(sentiment_analysis_flair)\n",
    "\n",
    "print(f\"Consensus TextBlob - VADER: {accuracy_score(temp['blob_sentiment'],temp['vader_sentiment']):.4}\")\n",
    "print(f\"Consensus TextBlob - Flair: {accuracy_score(temp['blob_sentiment'],temp['flair_sentiment']):.4}\")\n",
    "print(f\"Consensus VADER - Flair: {accuracy_score(temp['vader_sentiment'],temp['flair_sentiment']):.4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwYU9ZodxHqK"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_title('Agreement between TextBlob, VADER and Flair predictions', fontsize=16)\n",
    "\n",
    "features = np.array([1,2,3])\n",
    "values = [accuracy_score(temp['blob_sentiment'],temp['vader_sentiment']), accuracy_score(temp['blob_sentiment'],temp['flair_sentiment']), accuracy_score(temp['vader_sentiment'],temp['flair_sentiment'])]\n",
    "\n",
    "ax.bar(features, values, width=0.3, align = 'center', color = 'g', alpha= .6)\n",
    "\n",
    "ax.set_ylim(0, .6)\n",
    "#ax.set_xlabel('Valence', fontsize = 14)\n",
    "ax.set_ylabel('% of agreement', fontsize = 14)\n",
    "\n",
    "ax.set_xticklabels(['', 'Consensus TextBlob\\n-VADER', '', 'Consensus TextBlob\\n-Flair', '', 'Consensus VADER\\n- Flair'])\n",
    "\n",
    "#labels= [\"Trump\", \"Biden\"]\n",
    "#ax.legend(labels)\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.80])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_GyRbdtxPTE"
   },
   "source": [
    "The results above indicate two possibilities:\n",
    "\n",
    "- either all algorithms are wrong a lot of times\n",
    "- or just two of them are mostly wrong and one does a good job.\n",
    "\n",
    "Unfortunately, I know of no other way to automatically compare performance, so we will visualize a few examples where they all disagree and try to eyeball who's the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHvqg5XgxMJ0"
   },
   "outputs": [],
   "source": [
    "def consensus(row):\n",
    "    count = 0\n",
    "    count += row['blob_sentiment']==row['vader_sentiment']\n",
    "    count += row['blob_sentiment']==row['flair_sentiment']\n",
    "    count += row['vader_sentiment']==row['flair_sentiment']\n",
    "\n",
    "    return count\n",
    "\n",
    "temp['consensus'] = temp.apply(lambda row: consensus(row), axis=1)\n",
    "\n",
    "print(temp['consensus'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEnDy0ICxV5Y"
   },
   "outputs": [],
   "source": [
    "# Change the number of displayd results in the code below if you want to visually inspect more of them yourself\n",
    "(temp[temp['consensus']==0])[['tweet', 'clean_tweet_nltk', 'blob_sentiment', 'vader_sentiment', 'flair_sentiment']].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYdIsPVBxft5"
   },
   "source": [
    "## Based on the distribution plots and after inspecting a few tweets where all three algorithms applied different labels, I decided to use in further analyses the results from VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9u8FpCEBxX8k"
   },
   "outputs": [],
   "source": [
    "## Actionable insights from sentiment analysis of tweets¶\n",
    "There are several ways to use sentiment analysis on tweets related to a political campaign.\n",
    "\n",
    "One is to try to predict the election results from the sentiments towards the candidated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irvmZfczxsHV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRiHvbwpxdxP"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
