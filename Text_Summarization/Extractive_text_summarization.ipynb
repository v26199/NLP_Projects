{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# An Extractive summarization method consists of selecting important sentences, paragraphs etc. from the original document and concatenating them into shorter form.\n",
        "\n",
        "## How to do text summarization\n",
        "\n",
        "* Text cleaning\n",
        "* Sentence Tokenization\n",
        "* Word tokenization\n",
        "* Word-frequency table\n",
        "* Text Summarization"
      ],
      "metadata": {
        "id": "rDoPVyJRz_Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text\n",
        "\n",
        "text = \"\"\"\n",
        "\n",
        "Text Summarization using NLP\n",
        "Published by georgiannacambel on 4 September 2020\n",
        "Extractive Text Summarization\n",
        "What is text summarization?\n",
        "Text summarization is the process of creating a short, accurate, and fluent summary of a longer text document. It is the process of distilling the most important information from a source text. Automatic text summarization is a common problem in machine learning and natural language processing (NLP). Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online to both better help discover relevant information and to consume relevant information faster.\n",
        "\n",
        "Why automatic text summarization?\n",
        "Summaries reduce reading time.\n",
        "While researching using various documents, summaries make the selection process easier.\n",
        "Automatic summarization improves the effectiveness of indexing.\n",
        "Automatic summarization algorithms are less biased than human summarizers.\n",
        "Personalized summaries are useful in question-answering systems as they provide personalized information.\n",
        "Using automatic or semi-automatic summarization systems enables commercial abstract services to - increase the number of text documents they are able to process.\n",
        "\n",
        "An Extractive summarization method consists of selecting important sentences, paragraphs etc. from the original document and concatenating them into shorter form.\n",
        "An Abstractive summarization is an understanding of the main concepts in a document and then express those concepts in clear natural language.\n",
        "The Domain-specific summarization techniques utilize the available knowledge specific to the domain of text. For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.\n",
        "The Generic summarization focuses on obtaining a generic summary or abstract of the collection of documents, or sets of images, or videos, news stories etc.\n",
        "TheQuery-based summarization, sometimes called query-relevant summarization, summarizes objects specific to a query.\n",
        "The Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents.\n",
        "The Single-document summarization generates a summary from a single source document.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yvRptucF0BpB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Get Started with SpaCy\n",
        "!pip install -U spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "\"\"\"\n",
        "spacy for Natural Language Processing.\n",
        "STOP_WORDS is a set of default stop words for English language model in SpaCy.\n",
        "punctuation is a pre-initialized string which will give the all sets of punctuation.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s0ZkQZ403iW",
        "outputId": "5b24a37b-3ef5-4710-9b7c-0b923aa6ca66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "rMauODny1FhR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = list(STOP_WORDS)"
      ],
      "metadata": {
        "id": "fTsqSCmv1Ptq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list of stop words. -- > we can add more stop words mannualy\n",
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vnZsyor1RrP",
        "outputId": "e4b5c2c0-8f81-498f-b47a-0224febe7774"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['even', 'a', 'around', 'from', 'whereas', 'wherever', 'upon', 'within', 'thereafter', 'really', 'besides', 'than', 'show', '’m', 'various', 'should', 'never', 'some', '’ve', 'somehow', 'serious', 'again', 'same', 'except', 'rather', 'nine', 'namely', 'seem', 'whoever', 'five', 'nowhere', 'them', 'beyond', 'twelve', 'very', 'their', 'being', 'by', 'latterly', '’re', 'hereupon', 'using', 'ten', 'thus', 'enough', 'quite', 'third', 'toward', 'last', 'say', 'he', 'see', 'these', \"'d\", 'does', 'doing', 'him', 'top', 'on', 'be', 'an', 'above', 'up', '‘d', 'or', 'others', 'his', 'many', 'thence', 'did', 'whenever', 'here', 'six', 'only', 'few', 'until', 'all', 'were', 'behind', 'bottom', 'ours', 'yourself', 'n’t', 'why', 'cannot', 'forty', 'becomes', 'yours', 'also', 'myself', 'moreover', 'take', 'three', 'where', 'across', 'nor', 'per', 'thru', 'will', 'already', 'since', 'therefore', 'mostly', 'whereafter', \"'m\", 'due', 'n‘t', 'almost', 'me', 'been', 'along', 'into', 'too', 'latter', 'seemed', 'but', 'other', 'such', 'yourselves', 'would', 'former', 'seems', 'for', 'least', 'of', 'our', 'still', 'hereby', 'amongst', 'via', 'ourselves', \"'re\", 'eleven', 'any', 'both', 'often', 'are', 'ca', 'much', 'while', 'my', \"'ve\", 'keep', 'nevertheless', 'it', 'became', 'somewhere', 'two', 'twenty', 'none', 'its', 'whereby', 'which', 'was', 'might', 'beside', 'seeming', 'throughout', 'several', \"'ll\", 'then', 'must', 'elsewhere', 'whither', 'am', 'itself', 'always', 'made', 'once', 'put', 'done', 'onto', 'without', 'because', 'regarding', 'now', 'towards', '‘ve', 'those', 'full', 'anyone', 'however', 'thereby', 'how', 'before', 'if', 'has', 'alone', 'make', 'during', '’s', 'they', 'else', 'meanwhile', 'less', 'there', 'another', 'whatever', 'together', 'otherwise', 'can', 'down', 'eight', 'most', 'part', 'front', 'her', 'may', 'beforehand', 're', 'have', 'either', 'hence', 'unless', 'anything', 'thereupon', 'next', 'over', 'call', 'everywhere', 'name', 'perhaps', 'out', 'formerly', 'i', 'whether', 'between', 'side', 'anyhow', 'as', '‘s', '‘ll', 'give', 'ever', 'anywhere', 'afterwards', 'no', '’ll', \"n't\", 'to', 'your', 'with', 'get', 'yet', 'more', 'please', 'something', 'back', 'sometimes', 'off', 'fifteen', 'someone', 'not', 'that', 'himself', 'own', 'whom', 'nobody', 'fifty', 'becoming', 'herself', '’d', 'everyone', 'indeed', 'you', 'us', 'below', 'hundred', 'whose', 'after', 'the', 'each', 'and', \"'s\", 'do', 'used', 'who', 'we', 'further', 'first', 'herein', 'amount', 'among', 'mine', 'whole', 'hers', 'one', '‘re', 'what', 'at', 'well', 'just', 'four', 'is', 'sometime', 'wherein', 'anyway', 'in', 'could', 'every', 'had', 'therein', 'so', 'whence', 'under', 'noone', 'hereafter', '‘m', 'when', 'she', 'against', 'neither', 'themselves', 'although', 'move', 'become', 'about', 'empty', 'sixty', 'though', 'through', 'everything', 'go', 'whereupon', 'this', 'nothing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load nlp model\n",
        "\"\"\"\n",
        "spacy.load is used to load a model. spacy.load('en_core_web_sm') loads the model package en_core_web_sm.\n",
        "This will return a language object nlp containing all components and data needed to process text.\n",
        "\"\"\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "lefBVcFO1XkY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "cEZfM5ID1hcq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the document\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ4esSBa1qEw",
        "outputId": "531d4e1a-a0a1-4ce2-f431-b35a0e68a5dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n\\n', 'Text', 'Summarization', 'using', 'NLP', '\\n', 'Published', 'by', 'georgiannacambel', 'on', '4', 'September', '2020', '\\n', 'Extractive', 'Text', 'Summarization', '\\n', 'What', 'is', 'text', 'summarization', '?', '\\n', 'Text', 'summarization', 'is', 'the', 'process', 'of', 'creating', 'a', 'short', ',', 'accurate', ',', 'and', 'fluent', 'summary', 'of', 'a', 'longer', 'text', 'document', '.', 'It', 'is', 'the', 'process', 'of', 'distilling', 'the', 'most', 'important', 'information', 'from', 'a', 'source', 'text', '.', 'Automatic', 'text', 'summarization', 'is', 'a', 'common', 'problem', 'in', 'machine', 'learning', 'and', 'natural', 'language', 'processing', '(', 'NLP', ')', '.', 'Automatic', 'text', 'summarization', 'methods', 'are', 'greatly', 'needed', 'to', 'address', 'the', 'ever', '-', 'growing', 'amount', 'of', 'text', 'data', 'available', 'online', 'to', 'both', 'better', 'help', 'discover', 'relevant', 'information', 'and', 'to', 'consume', 'relevant', 'information', 'faster', '.', '\\n\\n', 'Why', 'automatic', 'text', 'summarization', '?', '\\n', 'Summaries', 'reduce', 'reading', 'time', '.', '\\n', 'While', 'researching', 'using', 'various', 'documents', ',', 'summaries', 'make', 'the', 'selection', 'process', 'easier', '.', '\\n', 'Automatic', 'summarization', 'improves', 'the', 'effectiveness', 'of', 'indexing', '.', '\\n', 'Automatic', 'summarization', 'algorithms', 'are', 'less', 'biased', 'than', 'human', 'summarizers', '.', '\\n', 'Personalized', 'summaries', 'are', 'useful', 'in', 'question', '-', 'answering', 'systems', 'as', 'they', 'provide', 'personalized', 'information', '.', '\\n', 'Using', 'automatic', 'or', 'semi', '-', 'automatic', 'summarization', 'systems', 'enables', 'commercial', 'abstract', 'services', 'to', '-', 'increase', 'the', 'number', 'of', 'text', 'documents', 'they', 'are', 'able', 'to', 'process', '.', '\\n\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKAjtBkK1wlk",
        "outputId": "7b3cb001-0371-449a-fe87-f757a51e405b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Perform text cleaning\n",
        "#1> remove stop punctuation\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "punctuation = punctuation + '\\n'\n",
        "punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tdLSgI8m1z0r",
        "outputId": "29a14307-fb8a-47e0-ac68-e106fe040abf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make wordfrequence table By ignoreing stop words, and punctuation.\n",
        "\n",
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "    if word.text.lower() not in stopwords:\n",
        "        if word.text.lower() not in punctuation:\n",
        "            if word.text not in word_frequencies.keys():\n",
        "                word_frequencies[word.text] = 1\n",
        "            else:\n",
        "                word_frequencies[word.text] += 1\n",
        "\n",
        "print(word_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JNuRghV2FLx",
        "outputId": "09414e65-6747-45dc-8a27-3d35bc2af97d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n\\n': 3, 'Text': 3, 'Summarization': 2, 'NLP': 2, 'Published': 1, 'georgiannacambel': 1, '4': 1, 'September': 1, '2020': 1, 'Extractive': 1, 'text': 8, 'summarization': 8, 'process': 4, 'creating': 1, 'short': 1, 'accurate': 1, 'fluent': 1, 'summary': 1, 'longer': 1, 'document': 1, 'distilling': 1, 'important': 1, 'information': 4, 'source': 1, 'Automatic': 4, 'common': 1, 'problem': 1, 'machine': 1, 'learning': 1, 'natural': 1, 'language': 1, 'processing': 1, 'methods': 1, 'greatly': 1, 'needed': 1, 'address': 1, 'growing': 1, 'data': 1, 'available': 1, 'online': 1, 'better': 1, 'help': 1, 'discover': 1, 'relevant': 2, 'consume': 1, 'faster': 1, 'automatic': 3, 'Summaries': 1, 'reduce': 1, 'reading': 1, 'time': 1, 'researching': 1, 'documents': 2, 'summaries': 2, 'selection': 1, 'easier': 1, 'improves': 1, 'effectiveness': 1, 'indexing': 1, 'algorithms': 1, 'biased': 1, 'human': 1, 'summarizers': 1, 'Personalized': 1, 'useful': 1, 'question': 1, 'answering': 1, 'systems': 2, 'provide': 1, 'personalized': 1, 'semi': 1, 'enables': 1, 'commercial': 1, 'abstract': 1, 'services': 1, 'increase': 1, 'number': 1, 'able': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_frequency = max(word_frequencies.values())\n",
        "max_frequency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wsy8VrT2V_o",
        "outputId": "7230246d-9a0e-421a-f527-f2e376e5ff51"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize this frequency.\n",
        "\n",
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = word_frequencies[word]/max_frequency\n",
        "\n",
        "print(word_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EWxd-7B2ZFW",
        "outputId": "66b4e6bb-be28-404c-81ea-d000475231aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n\\n': 0.375, 'Text': 0.375, 'Summarization': 0.25, 'NLP': 0.25, 'Published': 0.125, 'georgiannacambel': 0.125, '4': 0.125, 'September': 0.125, '2020': 0.125, 'Extractive': 0.125, 'text': 1.0, 'summarization': 1.0, 'process': 0.5, 'creating': 0.125, 'short': 0.125, 'accurate': 0.125, 'fluent': 0.125, 'summary': 0.125, 'longer': 0.125, 'document': 0.125, 'distilling': 0.125, 'important': 0.125, 'information': 0.5, 'source': 0.125, 'Automatic': 0.5, 'common': 0.125, 'problem': 0.125, 'machine': 0.125, 'learning': 0.125, 'natural': 0.125, 'language': 0.125, 'processing': 0.125, 'methods': 0.125, 'greatly': 0.125, 'needed': 0.125, 'address': 0.125, 'growing': 0.125, 'data': 0.125, 'available': 0.125, 'online': 0.125, 'better': 0.125, 'help': 0.125, 'discover': 0.125, 'relevant': 0.25, 'consume': 0.125, 'faster': 0.125, 'automatic': 0.375, 'Summaries': 0.125, 'reduce': 0.125, 'reading': 0.125, 'time': 0.125, 'researching': 0.125, 'documents': 0.25, 'summaries': 0.25, 'selection': 0.125, 'easier': 0.125, 'improves': 0.125, 'effectiveness': 0.125, 'indexing': 0.125, 'algorithms': 0.125, 'biased': 0.125, 'human': 0.125, 'summarizers': 0.125, 'Personalized': 0.125, 'useful': 0.125, 'question': 0.125, 'answering': 0.125, 'systems': 0.25, 'provide': 0.125, 'personalized': 0.125, 'semi': 0.125, 'enables': 0.125, 'commercial': 0.125, 'abstract': 0.125, 'services': 0.125, 'increase': 0.125, 'number': 0.125, 'able': 0.125}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence tokenization\n",
        "\n",
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_XQ5n4V2fFK",
        "outputId": "313734e8-5de7-48a3-b2f1-bd3609e2e196"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "\n",
            "Text Summarization using NLP\n",
            "Published by georgiannacambel on 4 September 2020\n",
            "Extractive Text Summarization\n",
            "What is text summarization?\n",
            ", Text summarization is the process of creating a short, accurate, and fluent summary of a longer text document., It is the process of distilling the most important information from a source text., Automatic text summarization is a common problem in machine learning and natural language processing (NLP)., Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online to both better help discover relevant information and to consume relevant information faster.\n",
            "\n",
            ", Why automatic text summarization?\n",
            ", Summaries reduce reading time.\n",
            ", While researching using various documents, summaries make the selection process easier.\n",
            ", Automatic summarization improves the effectiveness of indexing.\n",
            ", Automatic summarization algorithms are less biased than human summarizers.\n",
            ", Personalized summaries are useful in question-answering systems as they provide personalized information.\n",
            ", Using automatic or semi-automatic summarization systems enables commercial abstract services to - increase the number of text documents they are able to process.\n",
            "\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Now we will calculate the sentence scores. The sentence score for a particular sentence is the sum of the normalized frequencies of the words in that sentence.\n",
        "All the sentences will be stored with their score in the dictionary sentence_scores.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dSPkmK0i2nOS",
        "outputId": "716dea60-4dc5-4a6e-afc1-c24ef9dfb1d3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNow we will calculate the sentence scores. The sentence score for a particular sentence is the sum of the normalized frequencies of the words in that sentence. \\nAll the sentences will be stored with their score in the dictionary sentence_scores.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "    for word in sent:\n",
        "        if word.text.lower() in word_frequencies.keys():\n",
        "            if sent not in sentence_scores.keys():\n",
        "                sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "            else:\n",
        "                sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
        "\n",
        "sentence_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qrQgwU-2uRa",
        "outputId": "b2496da0-f9a1-4e2f-c26b-dd9eb77d69b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              " \n",
              " Text Summarization using NLP\n",
              " Published by georgiannacambel on 4 September 2020\n",
              " Extractive Text Summarization\n",
              " What is text summarization?: 6.75,\n",
              " Text summarization is the process of creating a short, accurate, and fluent summary of a longer text document.: 4.375,\n",
              " It is the process of distilling the most important information from a source text.: 2.375,\n",
              " Automatic text summarization is a common problem in machine learning and natural language processing (NLP).: 3.25,\n",
              " Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online to both better help discover relevant information and to consume relevant information faster.\n",
              " : 6.875,\n",
              " Why automatic text summarization?: 2.375,\n",
              " Summaries reduce reading time.: 0.625,\n",
              " While researching using various documents, summaries make the selection process easier.: 1.375,\n",
              " Automatic summarization improves the effectiveness of indexing.: 1.75,\n",
              " Automatic summarization algorithms are less biased than human summarizers.: 1.875,\n",
              " Personalized summaries are useful in question-answering systems as they provide personalized information.: 1.75,\n",
              " Using automatic or semi-automatic summarization systems enables commercial abstract services to - increase the number of text documents they are able to process.\n",
              " : 5.125}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we are going to select 30% of the sentences having the largest scores. For this we are going to import nlargest from heapq.\n",
        "\n",
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "u4WKlJZS25yP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KETxb0-3HYO",
        "outputId": "2a1a037d-b253-4070-cee4-e2b995400378"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nlargest() will return a list with the select_length largest elements\n",
        "i.e. 4 largest elements from sentence_scores.\n",
        "key = sentence_scores.get specifies a function of one argument that is used to extract a comparison key from each element in sentence_scores\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_fIKrFG-3KtS",
        "outputId": "06569d9e-80a0-4156-ece9-1cc49138d43b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nnlargest() will return a list with the select_length largest elements \\ni.e. 4 largest elements from sentence_scores. \\nkey = sentence_scores.get specifies a function of one argument that is used to extract a comparison key from each element in sentence_scores\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8CbP_dJ3Y-5",
        "outputId": "cf7acf1c-0f62-44cf-fd7f-fb0c2279c4dc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online to both better help discover relevant information and to consume relevant information faster.\n",
              " ,\n",
              " \n",
              " \n",
              " Text Summarization using NLP\n",
              " Published by georgiannacambel on 4 September 2020\n",
              " Extractive Text Summarization\n",
              " What is text summarization?,\n",
              " Using automatic or semi-automatic summarization systems enables commercial abstract services to - increase the number of text documents they are able to process.\n",
              " ]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_summary = [word.text for word in summary]\n",
        "summary = ' '.join(final_summary)"
      ],
      "metadata": {
        "id": "wKGg1HXX3bt8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVFtF9KA3eGt",
        "outputId": "a4afcf73-d758-436d-9a48-2535ae398934"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Text Summarization using NLP\n",
            "Published by georgiannacambel on 4 September 2020\n",
            "Extractive Text Summarization\n",
            "What is text summarization?\n",
            "Text summarization is the process of creating a short, accurate, and fluent summary of a longer text document. It is the process of distilling the most important information from a source text. Automatic text summarization is a common problem in machine learning and natural language processing (NLP). Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online to both better help discover relevant information and to consume relevant information faster.\n",
            "\n",
            "Why automatic text summarization?\n",
            "Summaries reduce reading time.\n",
            "While researching using various documents, summaries make the selection process easier.\n",
            "Automatic summarization improves the effectiveness of indexing.\n",
            "Automatic summarization algorithms are less biased than human summarizers.\n",
            "Personalized summaries are useful in question-answering systems as they provide personalized information.\n",
            "Using automatic or semi-automatic summarization systems enables commercial abstract services to - increase the number of text documents they are able to process.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M04IlhsM3hAq",
        "outputId": "655d2e15-0671-4cfb-ca0f-458bb6c2699f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online to both better help discover relevant information and to consume relevant information faster.\n",
            "\n",
            " \n",
            "\n",
            "Text Summarization using NLP\n",
            "Published by georgiannacambel on 4 September 2020\n",
            "Extractive Text Summarization\n",
            "What is text summarization?\n",
            " Using automatic or semi-automatic summarization systems enables commercial abstract services to - increase the number of text documents they are able to process.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RyKnI8Yz3pIp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}