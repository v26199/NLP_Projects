{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UbIZu5JWyJv"
      },
      "source": [
        "# Language Detection using NLP tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QVH3EzSYPlk"
      },
      "source": [
        "## 1> Langdetect\n",
        "Requires large portions of text. It uses non-deterministic approach under the hood. That means you get different results for the same text sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl_0Qu_XW2mY",
        "outputId": "5a3b6fde-467a-43c7-8d09-e579ad5650c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language of text Bonjour tout le monde is: fr\n"
          ]
        }
      ],
      "source": [
        "# !pip install langdetect\n",
        "# ! pip install textblob\n",
        "\n",
        "\n",
        "example_tweet = '#Wisconsin podría ser el punto de inflexión en la carrera entre #Trump y #Biden https://t.co/WFf8A1hAn7'\n",
        "\n",
        "# testing\n",
        "example_tweet = '#Wisconsin podría ser el punto de inflexión en la carrera entre #Trump y #Biden https://t.co/WFf8A1hAn7'\n",
        "\n",
        "\n",
        "\n",
        "from langdetect import detect\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Example tweet\n",
        "example_tweet = \"Bonjour tout le monde\"\n",
        "\n",
        "# Detect language using langdetect\n",
        "detected_language = detect(example_tweet)\n",
        "print(f'Language of text { example_tweet} is: {detected_language}')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ueJ6DNIYZ7F"
      },
      "source": [
        "## 2>Textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Oss7bqHtW-At"
      },
      "outputs": [],
      "source": [
        "# # Proceed with TextBlob analysis\n",
        "# from textblob import TextBlob\n",
        "# b = TextBlob(\"bonjour\")\n",
        "# b.detect_language()\n",
        "\n",
        "# Note: This solution requires internet access and Textblob is using Google Translate's language detector by calling the API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdn3lamkYU9d"
      },
      "source": [
        "## 3> Polyglot.\n",
        "#### Able to detect texts with mixed languages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSZjkeh_Xqfw",
        "outputId": "6a840907-9827-46cd-c0d3-2188b3236ca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name: English     code: en       confidence:  87.0 read bytes:  1154\n",
            "name: Chinese     code: zh_Hant  confidence:   5.0 read bytes:  1755\n",
            "name: un          code: un       confidence:   0.0 read bytes:     0\n"
          ]
        }
      ],
      "source": [
        "# !pip install polyglot\n",
        "# !apt-get install -y python3-icu\n",
        "# !apt-get install -y libicu-dev\n",
        "# !pip install pyicu\n",
        "# !pip install pycld2\n",
        "\n",
        "from polyglot.detect import Detector\n",
        "\n",
        "mixed_text = u\"\"\"\n",
        "China (simplified Chinese: 中国; traditional Chinese: 中國),\n",
        "officially the People's Republic of China (PRC), is a sovereign state\n",
        "located in East Asia.\n",
        "\"\"\"\n",
        "for language in Detector(mixed_text).languages:\n",
        "        print(language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xvz4RNiaFZJ"
      },
      "source": [
        "## 4>  chardet\n",
        "#### Chardet has also a feature of detecting languages if there are character bytes in range (127-255]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFxC3OocYmEf",
        "outputId": "fb2187d2-8015-4564-cd6b-7ad0947487af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'encoding': 'windows-1251',\n",
              " 'confidence': 0.9637267119204621,\n",
              " 'language': 'Russian'}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !pip install chardet\n",
        "import chardet\n",
        "chardet.detect(\"Я люблю вкусные пампушки\".encode('cp1251'))\n",
        "{'encoding': 'windows-1251', 'confidence': 0.9637267119204621, 'language': 'Russian'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-mPNfwiaqmg"
      },
      "source": [
        "## 5> guess_language\n",
        "Can detect very short samples by using this spell checker with dictionaries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aR0hTK7aLhy",
        "outputId": "30300b6e-a64d-4907-c4e1-b0c96948044d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting guess_language-spirit\n",
            "  Downloading guess_language-spirit-0.5.3.tar.bz2 (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m71.7/81.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: guess_language-spirit\n",
            "  Building wheel for guess_language-spirit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for guess_language-spirit: filename=guess_language_spirit-0.5.3-py3-none-any.whl size=121197 sha256=09293b91e46d81707c99f471b0c673e2d14279ce4f0aa15aa8b8f75dbb68c6d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/32/34/62e25b4c55d2802bb7d6540aed1fe171722a4f3bd854986d89\n",
            "Successfully built guess_language-spirit\n",
            "Installing collected packages: guess_language-spirit\n",
            "Successfully installed guess_language-spirit-0.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install guess_language-spirit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcYGvjBtazRX"
      },
      "source": [
        "## 6> langid\n",
        "langid.py provides both a module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9S8AIBVauVA",
        "outputId": "de60e7ac-e269-4cae-b2e5-da41317d0d9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.25.2)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=e742ca9ba6ab66b6fe11f0c82d6900f66cffc40589d9267934300ae32aaea082\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ],
      "source": [
        "! pip install langid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlnSgsKfa5vV",
        "outputId": "667cc11e-74c8-4cc2-ab70-23c007f17c60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('en', -54.41310358047485)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import langid\n",
        "langid.classify(\"This is a test\")\n",
        "# ('en', -54.41310358047485)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSfVUUcgbD1I"
      },
      "source": [
        "## 7> FastText\n",
        "FastText is a text classifier, can be used to recognize 176 languages with a proper models for language classification.\n",
        "\n",
        "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKkmx4xNa93R",
        "outputId": "b4e0ec0e-4746-4587-abf9-909cecadae3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "# !pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNoV3tQ_bIOy",
        "outputId": "26e1c90c-ce4b-47ea-c9f5-82f951aef034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('__label__ar', '__label__fa')\n",
            "[0.98124713 0.01265871]\n",
            "Language: ar, Probability: 0.9812471270561218\n",
            "Language: fa, Probability: 0.012658712454140186\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "import fasttext\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained language identification model\n",
        "model = fasttext.load_model('lid.176.ftz')\n",
        "\n",
        "# Predict the top 2 languages for the given text\n",
        "text = \"الشمس تشرق\"\n",
        "predictions = model.predict(text, k=2)\n",
        "\n",
        "labels, probabilities = predictions\n",
        "print(labels)          # Output: ('__label__ar', '__label__fa')\n",
        "print(probabilities)   # Output: array([0.98124713, 0.01265871])\n",
        "\n",
        "# Display the results in a more readable format\n",
        "for label, probability in zip(labels, probabilities):\n",
        "    language = label.replace('__label__', '')\n",
        "    print(f\"Language: {language}, Probability: {probability}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF7LKTHGblst"
      },
      "source": [
        "## 8> pyCLD3\n",
        "pycld3 is a neural network model for language identification. This package contains the inference code and a trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "dxMyC7OjbUcc",
        "outputId": "c733056a-fdc9-4f9c-ad7f-2107ed432651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pycld3\n",
            "  Using cached pycld3-0.22.tar.gz (726 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pycld3\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pycld3 (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pycld3\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pycld3\n",
            "Failed to build pycld3\n",
            "\u001b[31mERROR: Could not build wheels for pycld3, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cld3'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-83b659698a5a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Import the cld3 module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcld3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Example text in a different language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cld3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install cld3 if not already installed\n",
        "!pip install pycld3\n",
        "\n",
        "# Import the cld3 module\n",
        "import cld3\n",
        "\n",
        "# Example text in a different language\n",
        "text = \"影響包含對氣候的變化以及自然資源的枯竭程度\"\n",
        "\n",
        "# Detect the language of the text\n",
        "result = cld3.get_language(text)\n",
        "\n",
        "# Print the detected language and its probability\n",
        "print(f\"Language: {result.language}, Probability: {result.probability}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwhNjoIAbqau"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
